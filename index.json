[{"content":"Summary Hoard is a concurrent memory allocator algorithm that achieves both a performance close to the uniprocessor allocator and a low memory fragmentation rate. False sharing and fragmentation are two showing problems that degrade the performance of concurrent memory allocators. Allocator may actively or passively cause false sharing by splitting a cache line to different processes in turn or by reusing the free pieces. On the other hand, internal and external fragmentation may cause unbounded memory blowup. Hoard assigns a local heap for each processor and maintains a global heap. All the memory in heaps is managed in a structure called superblock with the same size and exploits the locality by using LIFO free list. In the runtime, all the per-processor heaps would keep the emptiness under a given threshold. The freest superblock would be delivered to the global heap if the emptiness is too high. When calling malloc, the space of the lease free superblock would be used. Overall, with bounded memory overhead, Hoard make a concurrent allocator fast and salable.\nStrengths  As fast as a uniprocessor allocator; The memory consumption is theoretically and experimentally bounded; Largely alleviate the false sharing problem.  Weaknesses  Lock overhead and global heap contention; The false sharing problem has not been totally removed; Deliver the large objects management to virtual memory system.  How to improve  Design a lock-free heap framework that further reduce the overhead of synchronization, especially for single-thread cases; Internal fragmentation for smalle objects could be further reduced by a cleaning daemon.  Related work The following works also focus on improving the performance of concurrent memory allocator, but from different perspectives. LAMA (ATC ’15) [2] and Streamflow (ISMM ’06) [3] want to improve the performance by exploiting cache locality; This work (PLDI ’04) [4] uses a lock-free implementation to improve Hoard, and FreeBSD malloc(3) (2006) [5] shares the same idea; more recent work like Mallacc (ASPLOS ’17) [6] and SuperMalloc (ISMM ’15) [7] are pushing the performance to a new level.\nReference [1] Emery D. Berger, Kathryn S. McKinley, Robert D. Blumofe, and Paul R. Wilson. 2000. Hoard: a scalable memory allocator for multithreaded applications. In Proceedings of the ninth international conference on Architectural support for programming languages and operating systems (ASPLOS IX). ACM.\n[2] Hu, X., Wang, X., Li, Y., Zhou, L., Luo, Y., Ding, C., \u0026hellip; \u0026amp; Wang, Z. (2015). {LAMA}: Optimized Locality-aware Memory Allocation for Key-value Cache. In 2015 {USENIX} Annual Technical Conference ({USENIX}{ATC} 15) (pp. 57-69).\n[3] Schneider, S., Antonopoulos, C. D., \u0026amp; Nikolopoulos, D. S. (2006, June). Scalable locality-conscious multithreaded memory allocation. In Proceedings of the 5th international symposium on Memory management (pp. 84-94). ACM.\n[4] Michael, M. M. (2004, June). Scalable lock-free dynamic memory allocation. In ACM Sigplan Notices (Vol. 39, No. 6, pp. 35-46). ACM.\n[5] Evans, J. (2006, April). A scalable concurrent malloc (3) implementation for FreeBSD. In Proc. of the bsdcan conference, ottawa, canada.\n[6] Kanev, S., Xi, S. L., Wei, G. Y., \u0026amp; Brooks, D. (2017). Mallacc: Accelerating memory allocation. ACM SIGOPS Operating Systems Review, 51(2), 33-45.\n[7] Kuszmaul, B. C. (2015, June). SuperMalloc: a super fast multithreaded malloc for 64-bit machines. In ACM SIGPLAN Notices (Vol. 50, No. 11, pp. 41-55). ACM.\n","permalink":"https://blog.mhxie.me/review/hoard_asplosix/","summary":"Summary Hoard is a concurrent memory allocator algorithm that achieves both a performance close to the uniprocessor allocator and a low memory fragmentation rate. False sharing and fragmentation are two showing problems that degrade the performance of concurrent memory allocators. Allocator may actively or passively cause false sharing by splitting a cache line to different processes in turn or by reusing the free pieces. On the other hand, internal and external fragmentation may cause unbounded memory blowup.","title":"Hoard Review - ASPLOS IX"},{"content":"BDP(Bandwidth delay product): bandwidth-delay product (BDP\u0026rsquo;s) worth of buffer, where bandwidth is the bottleneck link and delay is the round-trip time (RTT) between sender and destination.\nOptimization more on the sender part?\nClient-driven protocol eRPC is a datacenter remote procedure call (RPC) framework or say library that provides high performance RPC based on either lossy ethernet or lossless fabrics.\nBased on the high speed packet IO framework DPDK, the main design of eRPC lies on the following optimizations:\n(1) Zero-copy request processing: assuming the common case of lossless network, in client end, eRPC will not allow a TX queue with a reference to the request msgbuf when that request is being processed, which means there should not be any retransmitted packet in their assumption.\n(2) Preallocated responses: following (1), in server end, eRPC does not need to copy data from RX buffer ring to dynamically allocated application buffer called msgbuf but only using DMAed data whose headers are stripped in advance.\n(3) Multi-packet RQ: they came up with a BDP flow control that limit the number of in-flight packets based on the allowable BDP/packet size (e.g. MTU)\nCommon cases optimizations: (4) Rate limiter bypass: it will directly place the data to rx/tx buffer in userspace rather than rate limiter.\n(5) Congestion control (timely) bypass: based on the report showing that data centers are usually underutilized and uncongested, they intentionally omit the congestion control until\n(6) Batched RTT time stamp: timestamping overhead can hurt a lot because each call to rdtsc() cost 8ns when it comes to a scale of million-level. So they batched this call by sampling.\nSo basically, eRPC is a composition of optimization ideas with the assumption of lossless and uncongested situation. As a result, eRPC achieves up to 10 mpps for small RPC request or 75Gbps for large messages by using a single core. What\u0026rsquo;s more, the replication latency is as low as 5.5us which is even better than programmable hardware options.\nFuture directions: Statistical multiplexing for session credits\nDrawbacks:  eRPC is pretty much optimized for small RPC request, the throughput result is based on a packet as large as 8MB which is a ridiculous size in most of cases. This paper does not show any diagram of the performance what if the common case is not satisfied. They do not implement SACKs due to the engineering complexity. Bypassing the rate limiter is wired and inconvenient in terms of performance isolation and it is by no means a good choise from the perspective of virtualization.  ","permalink":"https://blog.mhxie.me/review/erpc_nsdi19/","summary":"BDP(Bandwidth delay product): bandwidth-delay product (BDP\u0026rsquo;s) worth of buffer, where bandwidth is the bottleneck link and delay is the round-trip time (RTT) between sender and destination.\nOptimization more on the sender part?\nClient-driven protocol eRPC is a datacenter remote procedure call (RPC) framework or say library that provides high performance RPC based on either lossy ethernet or lossless fabrics.\nBased on the high speed packet IO framework DPDK, the main design of eRPC lies on the following optimizations:","title":"eRPC Review - NSDI '19 "},{"content":"The performance of networking and storage hardware has developed rapidly in recent years. Emerging new interfaces like RDMA and NVMe make user-level hardware access and asynchronous I/O possible, allowing upper applications to take full advantages of these high-performance hardware. However, the authors of Crail (Stuedi, P. et al., 2017) point out that most of the recent applications are too low-brewed and too focused on serving particular workloads to satisfy general task requirement.\nCrail is a user-level I/O architecture for Spark, an Apache data processing systems, fully utilising the high-performance networking and storage hardware from scratch. Usually, the bottom of a stack is where hardware integrations happen. This problem is also known as Deep Layering, one of the software-related I/O bottlenecks (Animesh Trivedi et al., 2016). In addition, Data Locality and Legacy Interfaces contribute to the difficulties of leveraging high-performance hardware in Spark and its analogs. The authors state that Crail has solved the problems listed above by using storage tiering and resource disaggregation over a high-performance RDMA network.\nThe targeting I/O operations that Crail focuses on is the temporary data, which was generated from data shuffling, broadcasting within jobs or shared across jobs. In fact, two of the most frequently used workloads supported by Spark would gain remarkable efficiency improvement by adopting the Crail architecture.\nThe authors establish that the actual overheads in Spark network operations are caused mainly by the software stack rather than networking hardware. They make a consequent conclusion, proved by their solid experiments, that overheads of running these engines could be reduced by the disaggregated environment. This finding further supports the research on flash storage disaggregation (Klimovic, A. et al., 2016), making lots of relevant thoughts explorable.\nReference [1] Stuedi, P., Trivedi, A., Pfefferle, J., Stoica, R., Metzler, B., Ioannou, N., \u0026amp; Koltsidas, I. (2017). Crail: A High-Performance I/O Architecture for Distributed Data Processing.Crail: A High-Performance I/O Architecture for Distributed Data Processing. IEEE Data Eng. Bull., 40(1), 38-49.\n[2] Klimovic, A., Litz, H., \u0026amp; Kozyrakis, C. (2017). Reflex: remote flash ≈ local flash. ACM SIGOPS Operating Systems Review, 51(2), 345-359.\n","permalink":"https://blog.mhxie.me/review/crail/","summary":"The performance of networking and storage hardware has developed rapidly in recent years. Emerging new interfaces like RDMA and NVMe make user-level hardware access and asynchronous I/O possible, allowing upper applications to take full advantages of these high-performance hardware. However, the authors of Crail (Stuedi, P. et al., 2017) point out that most of the recent applications are too low-brewed and too focused on serving particular workloads to satisfy general task requirement.","title":"Crail Review - IEEE Data Eng. Bull. Vol 40"},{"content":"Beckmann, N., Chen, H., \u0026amp; Cidon, A. (2018, April). LHD: Improving Cache Hit Rate by Maximizing Hit Density. In 15th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 18). USENIX} Association.\nLeast hit density (LHD) [1] is a brand-new cache replacement algorithm designed for key-value caches. An improvement to the cache hit rate of current distributed, in-memory key-value caches has shown more and more importance in demand of latency sensitive applications recently. However, some most popular used policies like least recently used (LRU) or first in first out (FIFO) have their own problems. For instance, The performance of LRU would be largely lowered down if cache pollution happens. Other complicated policies that introduce parameter tuning may cause unreliable performance and some mechanisms employ synchronized states which could increase the cost and decrease the throughput.\nHowever, the strategy for specific workload can be somehow improved. Beckmann, N., Chen, H., \u0026amp; Cidon, A. propose the LHD which would predict the expected hit-per-space (hit density) that is consumed by each object. The main idea of LHD is to monitor the probability of an object that would hit the cache during the lifetime and divide it by the resources it would consume. The basic evicting step, just like most of the algorithms in this area, is replacing the cache block that has the lowest hit density. In other words, the insight is to make small objects that hit frequently. The hit probability can be calculated by this simple formula: collected hits / (size * lifetime). What’s more, LHD would absorb the extra information about objects intuitively.\nIn order to prove the effectiveness of this algorithm, they implement a specific Memcached-based application named RankCache which includes the LHD ranking function. Besides hit density, RankCache can also support other arbitrary ranking functions. For management of memory, RankCache chooses to use slab allocation in order to make the performance of insertion and eviction is predictable. As a approximate scheme, RankCache has a high tolerance to loosely synchronized events and the aging information collecting from Memcached, and its allocator was succeeded to enforce delicate synchronizations.\nAccording to the data shown by the authors of LHD, RankCache would have much higher throughput than its competitors under specific hit ratio. After introducing information on tags, RankCache performs better. However, the eviction performance is not good in terms of SET operations. This performance gap may be somehow alleviated by the performance gain from the GET operations, which are much more time-consuming in most of the key-value cache systems.\nReference [1] Beckmann, N., Chen, H., \u0026amp; Cidon, A. (2018, April). LHD: Improving Cache Hit Rate by Maximizing Hit Density. In 15th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 18). USENIX} Association.\n","permalink":"https://blog.mhxie.me/review/lhd_nsdi18/","summary":"Beckmann, N., Chen, H., \u0026amp; Cidon, A. (2018, April). LHD: Improving Cache Hit Rate by Maximizing Hit Density. In 15th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 18). USENIX} Association.\nLeast hit density (LHD) [1] is a brand-new cache replacement algorithm designed for key-value caches. An improvement to the cache hit rate of current distributed, in-memory key-value caches has shown more and more importance in demand of latency sensitive applications recently.","title":"LHD Review - NSDI '18"},{"content":"The Paper[1] presents a remote-flash-accessing system called ReFlex, which provides comparable performance to accessing local Flash. The authors point out that remote access to hard disks and remote access to Flash using RDMA (Remote Direct Memory Access) are two common ways to offer flexibility and efficiency in a data center. However, they prove these existing approaches are facing two critical problems: the first is how to tradeoff between performance and cost, while the second is how to provide predictable performance without interferences. Thus they design this system to avoid the problems current systems may raise while still meeting the performance expectations.\nReFlex uses a tightly integrated data plane architecture to provide low latency and high throughput access to remote Flash. In the first place, the authors exploit the virtualization features in the hardware, though it is a software system, to transfer data and requests without copying them. Also, the polling-based execution model eliminates the interruptions of process. Thanks to the novel QoS scheduler, ReFlex serves different types of tenants to enforce SLOs (Service-Level Objectives), even for different data streams. By these means, the authors state that ReFlex is able to deal with thousands of tenants and network connections due to its high utilization to CPU cores.\nThe architecture of ReFlex is Client-Server model: server is responsible for executing the models mentioned above and clients are written for applications to access the servers. Moreover, the authors re-design the control plane that runs on every ReFlex server to balance the loads. ReFlex outreaches its competitors on tail latency, request/response throughput and CPU resources utilization, with well-behaved QoS scheduling and tenant isolations. Therefore, ReFlex can serve up to 850K IOPS per core over TCP/IP networking, with merely 21us addition over direct access to local Flash.\nReference:\n[1] Klimovic, A., Litz, H., \u0026amp; Kozyrakis, C. (2017). Reflex: remote flash ≈ local flash. ACM SIGOPS Operating Systems Review, 51(2), 345-359.\n","permalink":"https://blog.mhxie.me/review/reflex_asplos17/","summary":"The Paper[1] presents a remote-flash-accessing system called ReFlex, which provides comparable performance to accessing local Flash. The authors point out that remote access to hard disks and remote access to Flash using RDMA (Remote Direct Memory Access) are two common ways to offer flexibility and efficiency in a data center. However, they prove these existing approaches are facing two critical problems: the first is how to tradeoff between performance and cost, while the second is how to provide predictable performance without interferences.","title":"ReFlex Review - ASPLOS '17 "},{"content":"My name is Minghao Xie. I am a third-year computer engineering Ph.D. student at Baskin School of Engineering, UC Santa Cruz. Before I got my B.E. degree in Computer Science from Sichuan University, China.\nI am fortunate to be advised by Chen Qian and Heiner Litz. My research interests revolve around computer networks and systems. My current work focuses on flash storage disaggregation within data centers as part of the Center for Research in Storage Systems (CRSS) at UCSC. I enjoy building real systems that are scalable, deliver high performance, and easy to use.\nI love researching and coding. During the COVID pandemic, I spent most of my other part-time reading and in-door cycling for self-improvement. I speak Chinese and English, and I am learning Japanese.\nI have two adorable cats: Toby and Haruka. You may find them in my recent Instagram posts.\n","permalink":"https://blog.mhxie.me/about/","summary":"About","title":"About me"}]